{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BreastCancerNet Convolutional Neural Network (CNN) in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the necessary packages. \n",
    "\n",
    "`matplotlib` : We set matplotlib to use the \"Agg\"  backend so that we’re able to save our training plots to disk.\n",
    "\n",
    "`torch` : We’ll be taking advantage of the DataLoader , lr_scheduler , Adagrad  optimizer, convert vector to parameters, and one-hot encoder. \n",
    "\n",
    "`sklearn` : From scikit-learn we’ll need its implementation of a classification_report  and a confusion_matrix.\n",
    "\n",
    "`BreastCancerNet` : Import `BreastCancerNet` to train and evaluate it. We’ll also need our `config` to grab the `paths` to our training, validation, and testing data splits. \n",
    "\n",
    "`imutils` : We’ll be using the paths  module to grab paths to each of our images.\n",
    "\n",
    "`numpy` :for numerical processing with Python. \n",
    "\n",
    "Now that we’ve imported the required libraries and we’ve parsed command line arguments, let’s define training parameters including our training image paths and account for class imbalance:\n",
    "\n",
    "*Lines 20* define the number of training epochs, initial learning rate, and batch size.\n",
    "\n",
    "From there, we grab our training image paths and determine the total number of images in each of the splits (*Lines 23-26*).\n",
    "\n",
    "We’ll go ahead and compute the classWeight for our training data to account for class imbalance/skew (*Lines 29 - 32*). \n",
    "\n",
    "Data augmentation, a form of regularization, is important for nearly all deep learning experiments to assist with model generalization. The method purposely perturbs training examples, changing their appearance slightly, before passing them into the network for training. This partially alleviates the need to gather more training data, though more training data will rarely hurt your model. Our data augmentation object, `trainAug`  is initialized on *Lines 35-44*. As you can see, random rotations, shifts, shears, and flips will be applied to our data as it is generated. Rescaling our image pixel intensities to the range `[0, 1]` is handled by the trainAug  generator as well as the `valAug` generator defined on *Line 47*.\n",
    "\n",
    "Here we initialize the training (*Lines 50-56*), validation (*lines 59-65*), and testing (*lines 68-74*) generator. Each generator will provide batches of images on demand, as is denoted by the batch_size  parameter.\n",
    "\n",
    "Our model is initialized with the `Adagrad` optimizer on *Lines 77-78*.\n",
    "\n",
    "We then compile our model with a \"`binary_crossentropy`\"  loss  function (since we only have two classes of data), as well as learning rate decay (*Line 79*).\n",
    "\n",
    "Making a call to the Keras fit_generator method, our training process is initiated. Using this method, our image data can reside on disk and be yielded in batches rather than having the whole dataset in RAM throughout training. While not 100% necessary for today’s 5.8GB dataset, you can see how useful this is if you had a 200GB dataset, for example.\n",
    "\n",
    "After training is complete, we’ll evaluate the model on the testing data. *Line 93* make predictions on all of our testing data (again using a generator object).\n",
    "\n",
    "The highest prediction indices are grabbed for each sample (*Line 96*) and then a classification_report is printed conveniently to the terminal (*Line 99*).\n",
    "\n",
    "Then we compute the confusion_matrix and then derive the accuracy, sensitivity , and specificity  (*Lines 102-106*). The matrix and each of these values is then printed in our terminal (*Lines 109-112*).\n",
    "\n",
    "Finally, let’s generate and store our training plot (*Lines 115-126*) . Our training history plot consists of training/validation loss and training/validation accuracy. These are plotted over time so that we can spot over/underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('__call__', <function LevelMapper.__call__ at 0x00000247CC4D90D8>), ('__init__', <function LevelMapper.__init__ at 0x00000247CC4D9048>)]\n",
      "[('__call__', <function BalancedPositiveNegativeSampler.__call__ at 0x00000247CC594D38>), ('__init__', <function BalancedPositiveNegativeSampler.__init__ at 0x00000247CC594CA8>)]\n",
      "[('__init__', <function BoxCoder.__init__ at 0x00000247CC5A1558>), ('decode', <function BoxCoder.decode at 0x00000247CC5A1708>), ('decode_single', <function BoxCoder.decode_single at 0x00000247CC5A1798>), ('encode', <function BoxCoder.encode at 0x00000247CC5A15E8>), ('encode_single', <function BoxCoder.encode_single at 0x00000247CC5A1678>)]\n",
      "[('__call__', <function Matcher.__call__ at 0x00000247CC5A1318>), ('__init__', <function Matcher.__init__ at 0x00000247CC5A18B8>), ('set_low_quality_matches_', <function Matcher.set_low_quality_matches_ at 0x00000247CC5A14C8>)]\n",
      "[('__init__', <function ImageList.__init__ at 0x00000247CC5A13A8>), ('to', <function ImageList.to at 0x00000247CC5A1A68>)]\n",
      "[('__init__', <function Timebase.__init__ at 0x00000247CC72E048>)]\n",
      "[('__init__', <function VideoMetaData.__init__ at 0x00000247CC72EE58>)]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'datasets/idc\\\\training'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-19bb18667141>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbreastcancernet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBreastCancerNet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbreastcancernet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbreastcancernet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloaders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\Data Science\\DeepLearning\\Capstone Project 2\\breast-cancer-classification\\breastcancernet\\breastcancernet\\loaders.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Prepare training set, validation set and testing set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtrainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform_idc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mvalset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVAL_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform_idc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mtestset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTEST_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform_idc\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DS\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    206\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DS\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m     92\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[0;32m     93\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DS\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[1;34m(self, dir)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mNo\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \"\"\"\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'datasets/idc\\\\training'"
     ]
    }
   ],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Adagrad\n",
    "from torch.nn.utils import convert_parameters\n",
    "from torch.nn.functional import one_hot\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from breastcancernet import BreastCancerNet\n",
    "from breastcancernet import config\n",
    "from breastcancernet import loaders\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize our number of epochs, initial learning rate, and batch size\n",
    "NUM_EPOCHS=40; INIT_LR=1e-2; BS=32\n",
    "\n",
    "# determine the total number of image paths in training, validation, and testing directories\n",
    "trainPaths=list(paths.list_images(config.TRAIN_PATH))\n",
    "lenTrain=len(trainPaths)\n",
    "lenVal=len(list(paths.list_images(config.VAL_PATH)))\n",
    "lenTest=len(list(paths.list_images(config.TEST_PATH)))\n",
    "\n",
    "# account for skew in the labeled data\n",
    "trainLabels=[int(p.split(os.path.sep)[-2]) for p in trainPaths]\n",
    "trainLabels=np_utils.to_categorical(trainLabels)\n",
    "classTotals=trainLabels.sum(axis=0)\n",
    "classWeight=classTotals.max()/classTotals\n",
    "\n",
    "# initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(\n",
    "  rescale=1/255.0,\n",
    "  rotation_range=20,\n",
    "  zoom_range=0.05,\n",
    "  width_shift_range=0.1,\n",
    "  height_shift_range=0.1,\n",
    "  shear_range=0.05,\n",
    "  horizontal_flip=True,\n",
    "  vertical_flip=True,\n",
    "  fill_mode=\"nearest\")\n",
    "\n",
    "# initialize the validation (and testing) data augmentation object\n",
    "valAug=ImageDataGenerator(rescale=1 / 255.0)\n",
    "\n",
    "# initialize the training generator\n",
    "trainGen = trainAug.flow_from_directory(\n",
    "  config.TRAIN_PATH,\n",
    "  class_mode=\"categorical\",\n",
    "  target_size=(48,48),\n",
    "  color_mode=\"rgb\",\n",
    "  shuffle=True,\n",
    "  batch_size=BS)\n",
    "\n",
    "# initialize the validation generator\n",
    "valGen = valAug.flow_from_directory(\n",
    "  config.VAL_PATH,\n",
    "  class_mode=\"categorical\",\n",
    "  target_size=(48,48),\n",
    "  color_mode=\"rgb\",\n",
    "  shuffle=False,\n",
    "  batch_size=BS)\n",
    "\n",
    "# initialize the testing generator\n",
    "testGen = valAug.flow_from_directory(\n",
    "  config.TEST_PATH,\n",
    "  class_mode=\"categorical\",\n",
    "  target_size=(48,48),\n",
    "  color_mode=\"rgb\",\n",
    "  shuffle=False,\n",
    "  batch_size=BS)\n",
    "\n",
    "# initialize our CancerNet model and compile it\n",
    "model=CancerNet.build(width=48,height=48,depth=3,classes=2)\n",
    "opt=Adagrad(lr=INIT_LR,decay=INIT_LR/NUM_EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\n",
    "\n",
    "# fit the model\n",
    "M=model.fit_generator(\n",
    "  trainGen,\n",
    "  steps_per_epoch=lenTrain//BS,\n",
    "  validation_data=valGen,\n",
    "  validation_steps=lenVal//BS,\n",
    "  class_weight=classWeight,\n",
    "  epochs=NUM_EPOCHS)\n",
    "\n",
    "# reset the testing generator and then use our trained model to make predictions on the data\n",
    "print(\"Now evaluating the model\")\n",
    "testGen.reset()\n",
    "pred_indices=model.predict_generator(testGen,steps=(lenTest//BS)+1)\n",
    "\n",
    "# for each image in the testing set we need to find the index of the label with corresponding largest predicted probability\n",
    "pred_indices=np.argmax(pred_indices,axis=1)\n",
    "\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(testGen.classes, pred_indices, target_names=testGen.class_indices.keys()))\n",
    "\n",
    "# compute the confusion matrix and and use it to derive the raw accuracy, sensitivity, and specificity\n",
    "cm=confusion_matrix(testGen.classes,pred_indices)\n",
    "total=sum(sum(cm))\n",
    "accuracy=(cm[0,0]+cm[1,1])/total\n",
    "specificity=cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "sensitivity=cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "\n",
    "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
    "print(cm)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Specificity: {specificity}')\n",
    "print(f'Sensitivity: {sensitivity}')\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = NUM_EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0,N), M.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0,N), M.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0,N), M.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0,N), M.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on the IDC Dataset\")\n",
    "plt.xlabel(\"Epoch No.\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig('plot.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
