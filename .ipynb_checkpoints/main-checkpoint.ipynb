{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BreastCancerNet Convolutional Neural Network (CNN) in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the necessary packages. \n",
    "\n",
    "`matplotlib` : We set matplotlib to use the \"Agg\"  backend so that we’re able to save our training plots to disk.\n",
    "\n",
    "`torch` : We’ll be taking advantage of the DataLoader , lr_scheduler , Adagrad  optimizer, convert vector to parameters, and one-hot encoder. \n",
    "\n",
    "`sklearn` : From scikit-learn we’ll need its implementation of a classification_report  and a confusion_matrix.\n",
    "\n",
    "`BreastCancerNet` : Import `BreastCancerNet` to train and evaluate it. We’ll also need our `config` to grab the `paths` to our training, validation, and testing data splits. \n",
    "\n",
    "`OneCycleLR` : For determining the learning rate of each training batch, we’ll use a technique known as the 1 cycle policy. First outlined in Leslie Smith’s A disciplined approach to neural network hyper-parameters, the 1 cycle policy consists of training in two steps, first going from a low to high learning rate, second going from high to low. The result of this approach is a significantly reduced training time. At the time of writing there is an open pull request to implement the policy in PyTorch, but for now I will copy the code to onecyclelr.py\n",
    "\n",
    "`imutils` : We’ll be using the paths  module to grab paths to each of our images.\n",
    "\n",
    "`numpy` :for numerical processing with Python. \n",
    "\n",
    "Now that we’ve imported the required libraries and we’ve parsed command line arguments, let’s define training parameters including our training image paths and account for class imbalance:\n",
    "\n",
    "*Lines 20* define the number of training epochs, initial learning rate, and batch size.\n",
    "\n",
    "From there, we grab our training image paths and determine the total number of images in each of the splits (*Lines 23-26*).\n",
    "\n",
    "We’ll go ahead and compute the classWeight for our training data to account for class imbalance/skew (*Lines 29 - 32*). \n",
    "\n",
    "Data augmentation, a form of regularization, is important for nearly all deep learning experiments to assist with model generalization. The method purposely perturbs training examples, changing their appearance slightly, before passing them into the network for training. This partially alleviates the need to gather more training data, though more training data will rarely hurt your model. Our data augmentation object, `trainAug`  is initialized on *Lines 35-44*. As you can see, random rotations, shifts, shears, and flips will be applied to our data as it is generated. Rescaling our image pixel intensities to the range `[0, 1]` is handled by the trainAug  generator as well as the `valAug` generator defined on *Line 47*.\n",
    "\n",
    "Here we initialize the training (*Lines 50-56*), validation (*lines 59-65*), and testing (*lines 68-74*) generator. Each generator will provide batches of images on demand, as is denoted by the batch_size  parameter.\n",
    "\n",
    "Our model is initialized with the `Adagrad` optimizer on *Lines 77-78*.\n",
    "\n",
    "We then compile our model with a \"`binary_crossentropy`\"  loss  function (since we only have two classes of data), as well as learning rate decay (*Line 79*).\n",
    "\n",
    "Making a call to the Keras fit_generator method, our training process is initiated. Using this method, our image data can reside on disk and be yielded in batches rather than having the whole dataset in RAM throughout training. While not 100% necessary for today’s 5.8GB dataset, you can see how useful this is if you had a 200GB dataset, for example.\n",
    "\n",
    "After training is complete, we’ll evaluate the model on the testing data. *Line 93* make predictions on all of our testing data (again using a generator object).\n",
    "\n",
    "The highest prediction indices are grabbed for each sample (*Line 96*) and then a classification_report is printed conveniently to the terminal (*Line 99*).\n",
    "\n",
    "Then we compute the confusion_matrix and then derive the accuracy, sensitivity , and specificity  (*Lines 102-106*). The matrix and each of these values is then printed in our terminal (*Lines 109-112*).\n",
    "\n",
    "Finally, let’s generate and store our training plot (*Lines 115-126*) . Our training history plot consists of training/validation loss and training/validation accuracy. These are plotted over time so that we can spot over/underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Adagrad\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from torch.nn.utils import convert_parameters\n",
    "from onecyclelr import OneCycleLR\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from breastcancernet import BreastCancerNet\n",
    "import config\n",
    "import loaders\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our number of epochs, initial learning rate, and batch size\n",
    "num_epochs=40; lr=1e-2; batch_size=32; num_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainloader=loaders.trainloader\n",
    "valloader = loaders.valloader\n",
    "testloader = loaders.testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We would like to use GPU for training if possible to speed up training process\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BreastCancerNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.23, inplace=False)\n",
       "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.25, inplace=False)\n",
       "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=6912, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=6912, out_features=6912, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=6912, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize our CancerNet model\n",
    "model= BreastCancerNet.BreastCancerNet()\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32, 3, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 32, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128, 64, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([6912, 4096])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([6912])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([6912, 6912])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([6912])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([2, 6912])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(type(param), param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to train a batch of IDC images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(trainloader, model, criterion, optimizer, scheduler):\n",
    "    total_loss = 0.0\n",
    "    size = len(trainloader.dataset)\n",
    "    num_batches = size // trainloader.batch_size\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        print(f\"Training: {i}/{num_batches}\", end=\"\\r\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images) # forward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        loss.backward()  # backprogagation\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss / size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to compute the accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valloader, model, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        total_loss = 0.0\n",
    "        size = len(valloader.dataset)\n",
    "        num_batches = size // valloader.batch_size\n",
    "        for i, (images, labels) in enumerate(valloader):\n",
    "            print(f\"Validation: {i}/{num_batches}\", end=\"\\r\")\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_correct += torch.sum(preds == labels.data)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            \n",
    "        return total_loss / size, total_correct.double() / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main training loop \n",
    "target_size=torch.rand((48,48), requires_grad=False)\n",
    "input_size=torch.rand((48,48), requires_grad=False)\n",
    "\n",
    "def fit(model, num_epochs, trainloader, valloader):\n",
    "    criterion = binary_cross_entropy(input_size, target_size) \n",
    "    optimizer = Adagrad(model.parameters(), lr=lr,lr_decay=lr/num_epochs)\n",
    "    scheduler = OneCycleLR(optimizer, lr_range=(lr,1.), num_steps=1000)\n",
    "    print(\"epoch\\ttrain loss\\tvalid loss\\taccuracy\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(trainloader, model, criterion, optimizer, scheduler)\n",
    "        valid_loss, valid_acc = validate(valloader, model, criterion)\n",
    "        print(f\"{epoch}\\t{train_loss:.5f}\\t\\t{valid_loss:.5f}\\t\\t{valid_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s train for 40 epochs and print the training loss, validation loss, and accuracy improve with each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\ttrain loss\tvalid loss\taccuracy\n",
      "Training: 0/2246\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-43b0d2fa479e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-40e2f7858ce2>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, num_epochs, trainloader, valloader)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch\\ttrain loss\\tvalid loss\\taccuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{epoch}\\t{train_loss:.5f}\\t\\t{valid_loss:.5f}\\t\\t{valid_acc:.3f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-5d375c701d85>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(trainloader, model, criterion, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# backprogagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "fit(model, 40, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(testGen.classes, pred_indices, target_names=testGen.class_indices.keys()))\n",
    "\n",
    "# compute the confusion matrix and and use it to derive the raw accuracy, sensitivity, and specificity\n",
    "cm=confusion_matrix(testGen.classes,pred_indices)\n",
    "total=sum(sum(cm))\n",
    "accuracy=(cm[0,0]+cm[1,1])/total\n",
    "specificity=cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "sensitivity=cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "\n",
    "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
    "print(cm)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Specificity: {specificity}')\n",
    "print(f'Sensitivity: {sensitivity}')\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = NUM_EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0,N), M.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0,N), M.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0,N), M.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0,N), M.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on the IDC Dataset\")\n",
    "plt.xlabel(\"Epoch No.\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig('plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def results(model, valloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    actual = []\n",
    "    with torch.no_grad():\n",
    "         for images, labels in valloader:\n",
    "            outputs = model(images.to(device))\n",
    "            preds.append(outputs.cpu()[:,1].numpy())\n",
    "            actual.append(labels.numpy())\n",
    "    return np.concatenate(preds), np.concatenate(actual)\n",
    "\n",
    "preds, actual = results(model, valloader)\n",
    "fpr, tpr, _ = roc_curve(actual, preds)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (area = {auc(fpr, tpr):.3f})\")\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristics'); plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
